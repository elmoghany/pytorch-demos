{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0b2812-ec4d-4f39-b1f2-b3269225d3aa",
   "metadata": {},
   "source": [
    "# 00. Pytorch Fundamentals    \n",
    "## Resource Notebook: https://www.learnpytorch.io/00_pytorch_fundamentals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d35e7704-dd9e-4e13-be0a-d1ca5cda28ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce965d6b-65be-46f6-a3e8-1e7e8dce134a",
   "metadata": {},
   "source": [
    "## introduction to tensors\n",
    "\n",
    "### Creating tensors\n",
    "\n",
    "#### pytorch tensors are created using 'torch.tensor()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b497cab2-ccb0-4dc4-ba20-e6d0781cd219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4853bdf-35ba-4b08-97f4-c521a6bf3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c7f1b0-007c-401f-b8ed-e806ee118d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tensor back as python int\n",
    "\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60d69b-e62a-4923-bace-af6dffb003a8",
   "metadata": {},
   "source": [
    "# Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d86444-fd80-4dc1-85b3-e2fb6d7d3f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vector = torch.tensor([7,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eef99c2-0011-427a-aa4d-61295b8145ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3a5027e-3142-4e70-8791-f490ac58365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb013f-093d-4c91-a8fc-dee7bd24543c",
   "metadata": {},
   "source": [
    "# Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18e8b048-0a93-4bd5-8249-ec14d5956dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MATRIX = torch.tensor([[7,8],\n",
    "                      [9,10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d6fbe03-37be-4088-bf91-a2d001928154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d265a982-6181-45f6-841b-ca8063786006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18e7637d-c9e3-457f-bf54-662005027a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433296cd-eb56-4b0e-9f13-174844153196",
   "metadata": {},
   "source": [
    "# tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46a95c86-a741-47e5-816e-740689e89d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                       [3,6,9],\n",
    "                       [2,4,5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ff85ec9-e3ca-4fb8-9ad6-f46cbc16430a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc8bacdf-560b-407d-8d92-ed25bf195dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6db03-d5bd-4ca2-a053-a189b128d056",
   "metadata": {},
   "source": [
    "# Random Tensors\n",
    "\n",
    "### Why random tensors are important?\n",
    "### because the way many neural networks learn is that\n",
    "### they start with tensors full of random numbers\n",
    "### and then adjust those random numbers\n",
    "### to better represent the data\n",
    "\n",
    "### start with random numbers -> look at data \n",
    "### -> update random numbers  -> look at data\n",
    "### -> update random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1928ed5c-f820-48b8-9bb4-d377d289987f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0453, 0.0857, 0.9704, 0.4541],\n",
       "         [0.9421, 0.4110, 0.0926, 0.1047],\n",
       "         [0.4684, 0.6457, 0.8882, 0.6364]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random tensor of size(3,4)\n",
    "\n",
    "random_tensor = torch.rand(1, 3,4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b9f3664-1b5b-46ec-9dfe-88685eb19e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b608658-6f4a-49a9-ae2b-83625fc73ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bb00f-9cdc-49ef-b6ef-b3e5d7e9ffdb",
   "metadata": {},
   "source": [
    "# create a random tensor with similar shape to an image tensor\n",
    "\n",
    "### height, width, number of color channels RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d73be591-416e-4efc-8ec3-b7933ffa2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_image_size_tensor = torch.rand(size=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "823e9c4d-fb7a-4073-a330-039ca47698be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.6246e-01, 7.6145e-01, 4.5105e-01],\n",
       "         [7.9098e-02, 8.9949e-01, 7.2903e-01],\n",
       "         [2.5651e-01, 5.1943e-01, 4.1376e-01],\n",
       "         ...,\n",
       "         [3.9056e-01, 9.3013e-01, 7.6522e-02],\n",
       "         [3.4133e-01, 5.4784e-01, 6.4806e-01],\n",
       "         [6.4371e-01, 9.4782e-02, 3.6684e-02]],\n",
       "\n",
       "        [[7.3617e-01, 2.1992e-01, 6.1007e-01],\n",
       "         [7.7685e-01, 9.1694e-01, 4.6254e-01],\n",
       "         [2.4760e-01, 5.8392e-02, 5.7244e-01],\n",
       "         ...,\n",
       "         [4.2240e-01, 8.6157e-02, 8.6074e-02],\n",
       "         [3.9452e-01, 1.1250e-01, 3.0502e-01],\n",
       "         [8.8254e-01, 8.2531e-01, 7.6113e-01]],\n",
       "\n",
       "        [[5.4145e-01, 7.2198e-01, 9.0374e-01],\n",
       "         [8.1715e-01, 5.8029e-01, 6.8607e-01],\n",
       "         [1.3420e-01, 3.2073e-01, 6.1109e-01],\n",
       "         ...,\n",
       "         [3.3283e-01, 6.2826e-01, 2.6579e-01],\n",
       "         [6.6087e-01, 4.4121e-01, 1.0489e-01],\n",
       "         [1.5790e-01, 5.8892e-01, 4.2023e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[4.7189e-01, 2.1143e-01, 6.9529e-04],\n",
       "         [6.0800e-01, 3.5320e-01, 8.9575e-01],\n",
       "         [6.6898e-01, 5.8532e-01, 3.0789e-01],\n",
       "         ...,\n",
       "         [4.3581e-01, 9.9977e-01, 9.2137e-01],\n",
       "         [6.8202e-01, 3.1223e-01, 2.6949e-01],\n",
       "         [8.5197e-03, 2.7986e-01, 7.1626e-01]],\n",
       "\n",
       "        [[9.3186e-01, 5.4379e-01, 9.9613e-01],\n",
       "         [8.7102e-01, 4.1025e-01, 4.0173e-01],\n",
       "         [2.4455e-01, 9.6406e-01, 1.4657e-02],\n",
       "         ...,\n",
       "         [1.3479e-01, 7.8647e-01, 9.7631e-01],\n",
       "         [9.8805e-01, 7.5934e-01, 8.0201e-02],\n",
       "         [4.1559e-01, 8.9096e-01, 1.9563e-01]],\n",
       "\n",
       "        [[3.9534e-02, 6.6234e-01, 6.9468e-01],\n",
       "         [1.1552e-01, 2.5165e-01, 8.6902e-01],\n",
       "         [6.1414e-02, 1.9760e-01, 5.3712e-01],\n",
       "         ...,\n",
       "         [9.3075e-01, 6.4475e-01, 8.5237e-01],\n",
       "         [5.6563e-01, 3.0257e-01, 3.1697e-01],\n",
       "         [3.0535e-01, 1.5468e-01, 6.6225e-01]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_size_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b34797f3-2c07-4976-972e-7b4ac10640e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_size_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84a2a1f3-2dcb-4906-b80f-9422c96fd08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_size_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4800915d-0a65-40fd-bb6a-a7ed0b65f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_size_tensor2 = torch.rand(224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "222810a5-50d4-453a-8399-2b1b98996952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1240, 0.2234, 0.1734],\n",
       "         [0.8295, 0.0709, 0.5530],\n",
       "         [0.4113, 0.8017, 0.7595],\n",
       "         ...,\n",
       "         [0.0837, 0.9166, 0.4565],\n",
       "         [0.9125, 0.8829, 0.0507],\n",
       "         [0.6996, 0.1848, 0.0932]],\n",
       "\n",
       "        [[0.7877, 0.3057, 0.2250],\n",
       "         [0.7203, 0.0327, 0.0381],\n",
       "         [0.4809, 0.4850, 0.6086],\n",
       "         ...,\n",
       "         [0.9007, 0.7279, 0.7286],\n",
       "         [0.2892, 0.8970, 0.7805],\n",
       "         [0.4281, 0.0897, 0.6700]],\n",
       "\n",
       "        [[0.9662, 0.0340, 0.3292],\n",
       "         [0.4376, 0.7191, 0.2174],\n",
       "         [0.3618, 0.0445, 0.3943],\n",
       "         ...,\n",
       "         [0.6255, 0.7393, 0.6305],\n",
       "         [0.7634, 0.5117, 0.4341],\n",
       "         [0.3726, 0.4208, 0.9180]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3198, 0.6075, 0.7710],\n",
       "         [0.7696, 0.3766, 0.2362],\n",
       "         [0.3458, 0.5616, 0.7820],\n",
       "         ...,\n",
       "         [0.0304, 0.8424, 0.7771],\n",
       "         [0.6100, 0.3686, 0.1903],\n",
       "         [0.8139, 0.6031, 0.5843]],\n",
       "\n",
       "        [[0.3906, 0.7728, 0.6288],\n",
       "         [0.7921, 0.9876, 0.9995],\n",
       "         [0.5852, 0.4585, 0.8364],\n",
       "         ...,\n",
       "         [0.0564, 0.8748, 0.5448],\n",
       "         [0.2817, 0.0644, 0.4870],\n",
       "         [0.0869, 0.1646, 0.6930]],\n",
       "\n",
       "        [[0.7177, 0.2536, 0.2704],\n",
       "         [0.1762, 0.9547, 0.3146],\n",
       "         [0.6851, 0.4227, 0.6980],\n",
       "         ...,\n",
       "         [0.1871, 0.2078, 0.9242],\n",
       "         [0.7529, 0.9536, 0.2609],\n",
       "         [0.5106, 0.1365, 0.7471]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_size_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e133d87b-4c75-4050-9598-437913c9ccb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_size_tensor2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b5217-be10-4f92-bc9c-16a4d6585459",
   "metadata": {},
   "source": [
    "# zeros and ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c832e3-0962-4998-b857-4772fd2ed5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zero = torch.zeros(3,4)\n",
    "zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f374df16-6f9c-4983-9f6b-eb74c66312d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(3,4)\n",
    "ones\n",
    "\n",
    "ones.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e81c8-da2e-497b-8ae1-22a88ddd04e5",
   "metadata": {},
   "source": [
    "# Create a range of tensors and tensors-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53ba523a-4e92-49d0-b091-958ddbbb7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_ten = torch.arange(0, 10)\n",
    "one_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfc8c9-36ad-4f5b-82c5-f50d9411a109",
   "metadata": {},
   "source": [
    "# creating tensors like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d7416-080b-4120-a33b-4f859a915280",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ten_zeros = torch.zeros_like(input=one_to_ten)\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740454ea-59b1-4d5f-b44f-7a9cf677c3b6",
   "metadata": {},
   "source": [
    "# Tensor data types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a060c87d-6002-44cc-839b-ec5b4c8afe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float 32 tensor\n",
    "\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None,  #what datatype is the tensor float16,32,64, complex32,128 => percision computing\n",
    "                               device=\"cpu\", #or cuda\n",
    "                               requires_grad=False)\n",
    "float_32_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba235db5-4dcb-40ba-b05c-2b43f45cdee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46ee2700-502f-4e56-a25c-abe5898c85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_16_tensor = float_32_tensor.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9459c5a8-d95f-4a83-9ebb-f0bea04a1b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], dtype=torch.float16)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "317af16f-1192-453a-996a-72a320bed21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_32_tensor = torch.tensor([3,6,9], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4df3577-c47f-4387-b76c-102a89dcdef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 36., 81.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor * int_32_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20f7666e-7252-4d74-abe9-68ce8a5f12c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47712641-2fc3-45f0-9aad-bc47b7c4f34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6dc7c06-5773-4d11-9d18-9f4e4a715dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da581a-4939-4133-9e7c-eb50d806b40e",
   "metadata": {},
   "source": [
    "# Manipulating Tensors (Tensor Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b2944-eec4-47b3-af83-c2091d0fd2be",
   "metadata": {},
   "source": [
    "- Addition\n",
    "- Subtraction\n",
    "- Multiplication\n",
    "- Dvision\n",
    "- Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bced5-1da4-4897-867f-fb94ba411e36",
   "metadata": {},
   "source": [
    "### create a tensor & add 10 to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4577c78c-07b2-4c11-b044-1553de8f8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "27a5b716-b3dc-4bee-b89f-c3287ac41cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503570ea-b149-4536-84e2-e634310b596c",
   "metadata": {},
   "source": [
    "### Multiply tensor by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a825da5b-8c1e-4314-9281-e5b45a62fdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d3175-fd80-4c38-a70b-571420fcd5e9",
   "metadata": {},
   "source": [
    "### subtract 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "836ec604-8570-4a2f-ae2f-7071f43fb683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b59d0b86-b3be-4e70-b6ec-a1abb836b619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch in-built functions\n",
    "torch.mul(tensor, 10)\n",
    "#torch.add()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99292ed2-9402-454f-b4ca-ab76dcd6df31",
   "metadata": {},
   "source": [
    "# Dot Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a63e1-4c02-49ab-a04c-6587c0240338",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1) Inner Dimension must match (3,2).(2,3) ... 2 & 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50df56-4b7b-4937-a4ac-898b20634cf5",
   "metadata": {},
   "source": [
    "### 2) Output equal to shape of Outer Dimension (3,2).(2,3) ... 3 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ac785-6a9e-4b6c-a133-1198597fd94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4402d65c-6b1b-412c-8b98-a43959c47fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element Wise Multiplication\n",
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f927a0a4-8f2f-4be3-88bf-47b27a26c8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842acc6-d327-4220-b6c8-62814ab566dc",
   "metadata": {},
   "source": [
    "## One of the most common errors in deep learning => shape error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c4029ca-112d-4bdc-ba68-2b1e517a484e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m tensor_A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      2\u001b[0m                          [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      3\u001b[0m                          [\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m]])\n\u001b[0;32m      4\u001b[0m tensor_B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m      5\u001b[0m                          [\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m11\u001b[39m],\n\u001b[0;32m      6\u001b[0m                          [\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m12\u001b[39m]])\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_B\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "tensor_A = torch.tensor([[1,2],\n",
    "                         [3,4],\n",
    "                         [5,6]])\n",
    "tensor_B = torch.tensor([[7,10],\n",
    "                         [8,11],\n",
    "                         [9,12]])\n",
    "torch.mm(tensor_A, tensor_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35782ebe-5e6a-4c3e-8f8e-9b9f2b1bc6a2",
   "metadata": {},
   "source": [
    "### To fix our tensor shape issues, we can manipulate the shape of one of our tensors using transpose\n",
    "### A transpose switches the axes or dimensions of a given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e94a0374-b288-453a-9fce-0a3a6bf02bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8,  9],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0c1f3-f9c7-4078-b23c-8f2d7b4ffeb1",
   "metadata": {},
   "source": [
    "# Tensor aggregation (finding the min, max, mean, sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6ddf6530-44b5-45f0-8069-e654293e6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "09412a50-3acf-40e3-9153-4300d812ad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc12f320-2ad0-47f0-8491-3561348c1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8cc1b07-19da-45b6-9e06-3be242d86048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x), x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bd6e57e3-95db-4f20-aeaf-856683568854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(90))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4222297-8be9-4b6d-ae3f-7f6e5a765110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x.type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "351eb22c-2ddd-4634-8896-20ba8572ef62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(450), tensor(450))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x), x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e6fbe-8541-42da-9d3f-806dce335a28",
   "metadata": {},
   "source": [
    "### finding the positional min \"argmin\" & max \"argmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0b17ef60-c97f-455c-8ab6-a9fcb9776f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "770e2c56-c7c4-4cdc-bafb-7d8812a81ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the position in tensor that has the min value with argmin -> returns index position where min value occurs\n",
    "x.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "925fb8ec-1f6c-43e6-95cb-d0da97bbbd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979981e-e91d-402a-a3d1-78927135ea46",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, squeezing, unsqueezing tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e19b5-6629-4be7-8215-f86ea7622c8c",
   "metadata": {},
   "source": [
    "- reshaping => reshapes an input tensor to a defined shape\n",
    "- view => return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
    "- stacking => combine multiple tensors on top of each other (vstack) or side by side (hstack)\n",
    "- squeeze => removes all '1' dim from a tensor\n",
    "- unsqueeze => add a '1' dim to a target tensor\n",
    "- premute => return a view of the input with dim permuted \"swapped\" in a certain way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f9fb59a9-4ff7-4b41-b71d-d8e622ea4f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1., 10.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b1ba8-8549-436c-92aa-105f4976a8a1",
   "metadata": {},
   "source": [
    "### add an extra dim \"reshape\"\n",
    "### - reshaping => reshapes an input tensor to a defined shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "62160489-1039-4ae9-a885-bc0c5db61113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3., 4., 5., 6., 7., 8., 9.]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.reshape(9) #same ... no change\n",
    "x_reshaped = x.reshape(1,9) #adding []\n",
    "x_reshaped = x.reshape(1,1,9) # adding two [[ ]]\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff660f8c-c17d-4fab-889b-726bc92889aa",
   "metadata": {},
   "source": [
    "### change the view \"view\"\n",
    "### - view => return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
    "#### changin z -> changes x (because a view of a tensor shares the same memory as the original input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "00ca20af-f903-49ac-86dd-9c6d8de8027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2e8824c7-293f-4495-b931-b19e6b71cce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(1,9)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "11257fa9-f654-4164-92bf-5f4acd7dd534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289defa3-d7f8-4d57-84b8-b0d558ea6ef2",
   "metadata": {},
   "source": [
    "### stack tensors on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d3a07043-5187-41f0-b4b8-abfc62c1f303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5., 5.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [5., 5., 5., 5.],\n",
       "        [6., 6., 6., 6.],\n",
       "        [7., 7., 7., 7.],\n",
       "        [8., 8., 8., 8.],\n",
       "        [9., 9., 9., 9.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stacked = torch.stack([x,x,x,x], dim=0)\n",
    "x_stacked = torch.stack([x,x,x,x], dim=1)\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e166f-7947-4cec-86a9-bf7f404e76e5",
   "metadata": {},
   "source": [
    "### torch.squeeze() - removes all single dimensions from a target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ed936dc6-6182-4dd2-aae9-e43e54671fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5., 2., 3., 4., 5., 6., 7., 8., 9.]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c2c89d79-fe65-4805-acb2-c201877e0afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 9])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "df957226-2baf-4a77-a3aa-a5685c9591e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_squeezed = x_reshaped.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8eb5cbd6-92da-4f44-949c-dadba7ac0c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf8840-3aed-451b-bc68-62d8f4db9eff",
   "metadata": {},
   "source": [
    "### torch unsqueeze() - adds a single dim to a target tensor at a specific dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fcaacdef-1d49-48dc-b9d1-4423aebb4657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3fd0da64-7a20-4f41-9ed1-6e2e3e3d0308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cdfc3261-f15e-4947-b288-a5ac40ce2fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "x_unsqueezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dcc0e298-43c5-4ccb-83bd-bfd6c53cdaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unsqueezed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d908c1-505e-446c-b359-ba1fd0d1890c",
   "metadata": {},
   "source": [
    "### torch.permute - rearranges the dimensions of a target tensor in a specified order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "17c1a641-1df7-4c5a-896b-393e349014be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([244, 244, 3])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_original = torch.rand(size=(244,244,3)) #height, width, color channels\n",
    "x_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "91757511-cd7a-4417-a9b3-e50ffddb8680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 244, 244])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_original.permute(2,0,1).shape #shifts axis 2->0 && 0->1 && 1->2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5575c4-e3ac-452b-b14d-d903e2921187",
   "metadata": {},
   "source": [
    "### indexing -> selecting data from tensors\n",
    "- indexing with pytorch is similar to indexing with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5531fada-9e5c-4c06-9c37-c51e82d4edc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8d89014c-fbfc-459e-827e-11f9fc123031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "dc41bf52-3efb-45ed-9ba3-697a5e091d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(1,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a5340a07-3fd3-4284-a094-96a793209ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ccab50eb-29a8-47be-a1e3-e75fe08b6f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] #index our new tensor \"dim=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b9a03676-ae93-4fbf-b278-5efe53b9c8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0] #index the middle bracket \"dim=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a4539d7e-4991-4cec-b098-3565c8fbae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0][0] #index the most inner bracket \"last dimension\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3557d75a-9f4e-43e2-bd93-341a8fb586e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use \":\" to select \"all\" of a target dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "315cc27e-f4e6-4f3e-977e-2bf651c5e3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "727d5876-9fbe-4cf9-b7e7-8984db156500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 7]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2a249d22-fbb3-45f6-ad1c-1ea971ce0dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,1] #gets all values of 0th and 1st dimension but only index 1 of 2nd dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "cf0ca6eb-756b-42d9-a970-5793e1350da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1, 1] #gets all values of the 0 dim but only the 1 index value of 1st & 2nd dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6bf82e1b-219c-4ee0-9881-1071ecfde1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index 0 of 0th and 1st dim and all values of 2nd dim\n",
    "x[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0431aa4d-2c62-4343-bc15-d9093e5fa34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 9])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index on x to return 9\n",
    "x[0,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a22053dc-171a-4cd5-883d-69653656899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index on x to return 3,6,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d38ab4de-8ca5-41b1-b803-8358d8e080af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 9])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69b6ad-bee2-4f16-8c4c-f45ec27e0bf6",
   "metadata": {},
   "source": [
    "# pytorch tensors & numpy\n",
    "### numpy is a popular scientific numerical computing library\n",
    "- data in numpy, want in pytorch tensor -> `torch.from_numpy(ndarray))\n",
    "- pytorch tensor -> numpy -> torch.Tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a181e8-d5ef-4891-aae9-610f0b8cf7e4",
   "metadata": {},
   "source": [
    "# numpy array => tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f52a7872-2f0e-4d23-a62b-8b92fae02853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array).type(torch.float32)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffdff4-1f30-4f06-859e-5572377869ca",
   "metadata": {},
   "source": [
    "### change the value of array, what will this do to tensor?\n",
    "### no change to tensor!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c1cb2fca-e7df-42d7-8898-7a50d3fadaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 4., 5., 6., 7., 8., 9.]), tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c1540-5f6d-4f77-8ad6-d5e58f3e5231",
   "metadata": {},
   "source": [
    "# tensor => numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "028b4a79-5ec8-4812-94be-419b5ca7edcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478836d-6e13-49ed-9ba1-57d29ad5ce66",
   "metadata": {},
   "source": [
    "# Reproducability (trying to take random out of random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc4fed-92e3-4143-9631-fb38e41ac1f8",
   "metadata": {},
   "source": [
    "- this is how neural network learns\n",
    "- starts with random numbers\n",
    "- => tensor operations\n",
    "- => update random numbers to try\n",
    "- => make them better representation of the data\n",
    "- => again\n",
    "- => again\n",
    "- to reduce the randomness => random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bc725fd0-8746-49dd-a14e-275be90c08b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
       "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
       "        [0.9408, 0.1332, 0.9346, 0.5936]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_a = torch.rand(3,4)\n",
    "random_tensor_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "38b4cf9c-0fcf-49f7-823b-86cceaa7878c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
       "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
       "        [0.9408, 0.1332, 0.9346, 0.5936]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_b = torch.rand(3,4)\n",
    "random_tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e16a8-9620-4bdd-8dcf-004a7550e8fd",
   "metadata": {},
   "source": [
    "# Running tensors & pytorch objects on GPUs => faster computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c0783-506c-4280-96f6-d84abefb5a08",
   "metadata": {},
   "source": [
    "### 1) Is GPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "55ce33e6-54ef-4408-bf14-e6ae3e856027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  3 12:55:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.13                 Driver Version: 537.13       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   48C    P8               4W /  78W |    139MiB /  6144MiB |     11%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     11432    C+G   C:\\Program Files (x86)\\SCM\\SCM.exe        N/A      |\n",
      "|    0   N/A  N/A     15672    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17429e56-fdce-415d-9702-00c7e0090245",
   "metadata": {},
   "source": [
    "### 2 check gpu access with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e5c95a9d-dddb-4eab-b77c-b426df55db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241b3dd-1a90-4328-9f7e-533d1f53d716",
   "metadata": {},
   "source": [
    "### 3 setup device agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4c1e5bd4-bb92-4fd9-a52a-6e4cccdbb0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebb9ea-a7b6-418d-a742-5dd908ecd76b",
   "metadata": {},
   "source": [
    "### 4 count number of devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1166c4b1-a308-4a64-8d37-57a471da0614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168fb72-f6d6-46f5-a2a2-a6f63710c4e4",
   "metadata": {},
   "source": [
    "# Putting tensors & models on GPU\n",
    "- the reason we want our tensors/models on the gpu because using gpu results in faster computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715202f-b727-4b88-8d19-6e32430ef56d",
   "metadata": {},
   "source": [
    "### create tensor (default on the cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c0479e03-d157-4dbf-8954-bb583f0203ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3], device=\"cpu\")\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "85da7795-3bc6-4abb-9469-38d789a22987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3])\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c5cb0-f50a-40f4-bb81-7f4b261f6814",
   "metadata": {},
   "source": [
    "### create tensor (on gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "cbe6da54-fdd3-471d-bbd4-b3353824adee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d9b5fa9f-cbf6-4959-99e5-f23e417ca89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3], device=\"cuda\")\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c4109-6bb0-440c-88b4-7fbc0c9fd6e7",
   "metadata": {},
   "source": [
    "### moving tensors back to cpu\n",
    "- if tensor on gpu => CAN NOT transform it to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2d740505-6eb5-41f6-b432-d402338ae096",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[259], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtensor_on_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3ece4b29-18e4-418e-8954-1f0ea7eab28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec7908-2a43-4789-92a4-290fa340c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### exercuses & extra-curriculum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
